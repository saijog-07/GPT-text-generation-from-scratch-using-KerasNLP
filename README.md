# GPT Text Generation Using KerasNLP

## Overview

This project aims to build a scaled-down Generative Pre-Trained (GPT) model using KerasNLP, a library tailored specifically for Natural Language Processing (NLP) tasks on top of Keras. The GPT model allows for sophisticated text generation from a prompt.

The model is trained on the `simplebooks-92` corpus, which is a dataset compiled from several novels. This dataset is suitable for this example due to its small vocabulary and high word frequency, which is beneficial when training a model with few parameters.

## Requirements

- Python 3.x
- KerasNLP
- TensorFlow

## Dataset

The simplebooks-92 dataset is a collection of novels compiled for training the GPT model. It is available for download at the following link:
[Download simplebooks.zip](https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip)


